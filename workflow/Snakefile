""" Snakemake pipeline for multi-run automated structure prediction. """

import os
import tempfile
from os.path import abspath, join, basename, splitext

include: "rules/common.smk"
configfile: "config/config.yaml"

# --- multi-run setup ---
RUNS = config["run"]
RUN_NAMES = [r["name"] for r in RUNS]
INPUT_PATHS = {r["name"]: abspath(r["input"]) for r in RUNS}
OUTPUT_PATHS = {r["name"]: abspath(r["output"]) for r in RUNS}

FEATURE_COMPRESSION = None
if config["create_feature_arguments"].get("--compress_features", False):
    FEATURE_COMPRESSION = "lzma"
FEATURE_SUFFIX = feature_suffix(FEATURE_COMPRESSION)
protein_delimiter = config.get("protein_delimiter", "_")

# parse all sample sheets together
with tempfile.NamedTemporaryFile(mode='w+', delete=False) as tmp_input:
    input_files = list(INPUT_PATHS.values())
    process_files(
        input_files=input_files,
        output_path=tmp_input.name,
        delimiter=protein_delimiter
    )
    dataset = InputParser.from_file(
        filepath=tmp_input.name,
        file_format="alphaabriss",
        protein_delimiter=protein_delimiter
    )

# --- compute required files ---
required_folds = expand(
    join(OUTPUT_PATHS[run], "predictions", fold, "completed_fold.txt"),
    run=RUN_NAMES,
    fold=dataset.fold_specifications
)
required_reports = (
    expand(
        join(OUTPUT_PATHS[run], "reports", "analysis.csv"),
        run=RUN_NAMES
    ) +
    expand(
        join(OUTPUT_PATHS[run], "reports", "report.html"),
        run=RUN_NAMES
    )
)
# feature files
required_features = expand(
    join(OUTPUT_PATHS[run], "features", f"{{fasta_basename}}.{FEATURE_SUFFIX}"),
    run=RUN_NAMES,
    fasta_basename=dataset.unique_sequences
)

# choose total inputs
if config.get("only_generate_features", False):
    total_required_files = required_features
else:
    total_required_files = required_folds + required_reports
    if config.get("cluster_jobs", False):
        total_required_files += expand(
            join(OUTPUT_PATHS[run], "resources", "sequence_clusters.txt"),
            run=RUN_NAMES
        )

# collect precomputed features
precomputed_features = []
feature_directories = config.get("feature_directory", [])
req_feat_names = set(basename(x) for x in required_features)
for feature_directory in feature_directories:
    if not os.path.exists(feature_directory):
        continue
    for available in os.listdir(feature_directory):
        if available in req_feat_names:
            precomputed_features.append(join(feature_directory, available))

rule all:
    input: total_required_files

rule symlink_local_files:
    input:
        lambda wc: dataset.sequences_by_origin["local"]
    output:
        lambda wc: [
            join(OUTPUT_PATHS[run], "data", f"{splitext(basename(x))[0]}.fasta")
            for run in RUN_NAMES
            for x in dataset.sequences_by_origin["local"]
        ]
    run:
        for run in RUN_NAMES:
            dataset.symlink_local_files(
                output_directory=join(OUTPUT_PATHS[run], "data")
            )

rule download_uniprot:
    output:
        lambda wc: [
            join(OUTPUT_PATHS[run], "data", f"{wc.uniprot_id}.fasta")
            for run in RUN_NAMES
        ]
    shell: """
        temp_file=$(mktemp)
        curl -o ${{temp_file}} https://rest.uniprot.org/uniprotkb/{wildcards.uniprot_id}.fasta
        for out in {output}; do
            echo ">{wildcards.uniprot_id}" > "$out"
            tail -n +2 ${{temp_file}} >> "$out"
        done
    """

rule symlink_features:
    input:
        lambda wc: precomputed_features
    output:
        lambda wc: [
            join(OUTPUT_PATHS[run], "features", basename(f))
            for run in RUN_NAMES
            for f in precomputed_features
        ]
    run:
        for src, dst in zip(input, output):
            if os.path.exists(dst):
                os.remove(dst)
            os.symlink(os.path.abspath(src), dst)

rule create_features:
    input:
        lambda wc: join(OUTPUT_PATHS[wc.run], "data", f"{wc.fasta_basename}.fasta")
    output:
        lambda wc: join(OUTPUT_PATHS[wc.run], "features", f"{wc.fasta_basename}.{FEATURE_SUFFIX}")
    params:
        data_directory = config["alphafold_data_directory"],
        output_directory = lambda wc: join(OUTPUT_PATHS[wc.run], "features"),
        cli_parameters = " ".join(
            [f"{k}={v}" for k, v in config["create_feature_arguments"].items()]
        )
    threads: 8
    container: config["prediction_container"]
    resources:
        mem_mb = lambda wc, attempt: config.get("feature_create_ram_bytes", 64000) * (config.get("feature_create_ram_scaling", 1.1) ** attempt),
        walltime = lambda wc, attempt: 1440 * attempt,
        attempt = lambda wc, attempt: attempt
    shell: """
        create_individual_features.py \
            --fasta_paths={input} \
            --data_dir={params.data_directory} \
            --output_dir={params.output_directory} \
            {params.cli_parameters}
    """

# --- sequence clustering checkpoint ---
checkpoint cluster_sequence_length:
    input:
        [join(OUTPUT_PATHS[run], "features", f"{feature}.{FEATURE_SUFFIX}") for run in RUN_NAMES for feature in dataset.unique_sequences]
    output:
        [join(OUTPUT_PATHS[run], "resources", "sequence_clusters.txt") for run in RUN_NAMES]
    params:
        folds = dataset.fold_specifications,
        protein_delimiter = protein_delimiter,
        feature_directory = lambda wc: join(OUTPUT_PATHS[wc.run], "features"),
        cluster_bin_size = config.get("clustering_bin_size", 150)
    threads: 1
    container: config["prediction_container"]
    resources:
        attempt = lambda wc, attempt: attempt,
        mem_mb = lambda wc, attempt: 800 * attempt,
        walltime = lambda wc, attempt: 10 * attempt
    script: "scripts/cluster_sequence_length.py"

# --- helpers for inference ---
def lookup_features(wc):
    if config.get("cluster_jobs", False):
        # update clustering data
        path = join(OUTPUT_PATHS[wc.run], "resources", "sequence_clusters.txt")
        with open(path) as f:
            data = [x.strip().split(",") for x in f if x.strip()]
        headers = data.pop(0)
        ret = {h: list(col) for h, col in zip(headers, zip(*data))}
        dataset.update_clustering(data=ret)
    return [
        join(OUTPUT_PATHS[wc.run], "features", f"{feature}.{FEATURE_SUFFIX}")
        for feature in dataset.sequences_by_fold[wc.fold]
    ]


def format_clustering(wc):
    params = ""
    if config.get("cluster_jobs", False):
        path = join(OUTPUT_PATHS[wc.run], "resources", "sequence_clusters.txt")
        with open(path) as f:
            data = [x.strip().split(",") for x in f if x.strip()]
        headers = data.pop(0)
        ret = {h: list(col) for h, col in zip(headers, zip(*data))}
        for name, length, depth in zip(ret["name"], ret["max_seq_length"], ret["max_msa_depth"]):
            if name == wc.fold:
                params += f"--desired_num_res={length} "
                params += f"--desired_num_msa={depth} "
    return params

rule structure_inference:
    input:
        features = lookup_features,
        cluster = lambda wc: join(OUTPUT_PATHS[wc.run], "resources", "sequence_clusters.txt") if config.get("cluster_jobs", False) else "/dev/null"
    output:
        lambda wc: join(OUTPUT_PATHS[wc.run], "predictions", wc.fold, "completed_fold.txt")
    params:
        data_directory = config["alphafold_data_directory"],
        feature_directory = lambda wc: join(OUTPUT_PATHS[wc.run], "features"),
        output_directory = lambda wc: join(OUTPUT_PATHS[wc.run], "predictions", wc.fold),
        requested_fold = lambda wc: wc.fold,
        protein_delimiter = protein_delimiter,
        clustering_format = format_clustering,
        cli_parameters = " ".join(
            [f"{k}={v}" for k, v in config["structure_inference_arguments"].items()]
        )
    threads: config["alphafold_inference_threads"]
    container: config["prediction_container"]
    resources:
        mem_mb = lambda wc, attempt: config.get("structure_inference_ram_bytes", 32000) * (1.1 ** attempt),
        walltime = lambda wc, attempt: 1440 * attempt,
        attempt = lambda wc, attempt: attempt
    shell: """
        run_structure_prediction.py \
            --input {params.requested_fold} \
            --output_directory={params.output_directory} \
            --protein_delimiter={params.protein_delimiter} \
            --data_directory={params.data_directory} \
            --features_directory={params.feature_directory} \
            {params.clustering_format} \
            {params.cli_parameters}
        echo "Completed" > "{output}"
    """

rule analyze_structure:
    input:
        lambda wc: join(OUTPUT_PATHS[wc.run], "predictions", wc.fold, "completed_fold.txt")
    output:
        lambda wc: join(OUTPUT_PATHS[wc.run], "predictions", wc.fold, "analysis.csv")
    params:
        prediction_dir = lambda wc: join(OUTPUT_PATHS[wc.run], "predictions", wc.fold),
        fold = lambda wc: wc.fold,
        report_dir = lambda wc: join(OUTPUT_PATHS[wc.run], "reports"),
        cli_parameters = " ".join(
            [f"{k}={v}" for k, v in config["analyze_structure_arguments"].items()]
        )
    container: config["analysis_container"]
    resources:
        mem_mb = lambda wc, attempt: 8000 * attempt,
        walltime = lambda wc, attempt: 1440 * attempt,
        attempt = lambda wc, attempt: attempt
    shell: """
        tmpdir=$(mktemp -d)
        cd $tmpdir
        ln -s {params.prediction_dir} $tmpdir/{params.fold}
        get_good_inter_pae.py \
            --output_dir=$tmpdir \
            {params.cli_parameters}
        mv predictions_with_good_interpae.csv {output}
        rm -rf $tmpdir
    """

rule merge_analyses:
    input:
        lambda wc: [
            join(OUTPUT_PATHS[run], "predictions", fold, "analysis.csv")
            for run in RUN_NAMES
            for fold in dataset.fold_specifications
        ]
    output:
        lambda wc: [
            join(OUTPUT_PATHS[run], "reports", "analysis.csv")
            for run in RUN_NAMES
        ]
    container: config["analysis_container"]
    resources:
        mem_mb = lambda wc, attempt: 2000 * attempt,
        walltime = lambda wc, attempt: 1440 * attempt,
        attempt = lambda wc, attempt: attempt
    shell: """
        head -n 1 {input[0]} > {output[0]}
        for f in {input}; do
            tail -n +2 "$f" >> {output[0]}
        done
    """

rule generate_report:
    input:
        lambda wc: [
            join(OUTPUT_PATHS[wc.run], "predictions", fold, "completed_fold.txt")
            for fold in dataset.fold_specifications
        ]
    output:
        lambda wc: join(OUTPUT_PATHS[wc.run], "reports", "report.html")
    params:
        prediction_dir = lambda wc: join(OUTPUT_PATHS[wc.run], "predictions"),
        report_dir = lambda wc: join(OUTPUT_PATHS[wc.run], "reports"),
        cli_parameters = " ".join(
            [f"{k}={v}" for k, v in config["generate_report_arguments"].items()]
        )
    container: config["prediction_container"]
    resources:
        mem_mb = lambda wc, attempt: 32000 * attempt,
        walltime = lambda wc, attempt: 1440 * attempt,
        attempt = lambda wc, attempt: attempt
    shell: """
        cd {params.prediction_dir}
        create_notebook.py \
            --output_dir={params.prediction_dir} \
            {params.cli_parameters}
        jupyter nbconvert --to html --execute output.ipynb
        mv output.ipynb {params.report_dir}
        mv output.html {output}
    """

