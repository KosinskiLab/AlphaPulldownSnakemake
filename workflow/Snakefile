""" Snakemake pipeline for automated structure prediction using various backends.

    Copyright (c) 2025 European Molecular Biology Laboratory

    Authors: Kosinski Lab <alphapulldown@embl-hamburg.de>
"""

from __future__ import annotations

configfile: "config/config.yaml"
include: "rules/common.smk"

from pathlib import Path
from os import makedirs, listdir, symlink, remove
from os.path import abspath, join, splitext, basename, exists

from alphapulldown_input_parser.parser import FoldDataset, generate_fold_specifications

config["output_directory"] = abspath(config["output_directory"])
makedirs(config["output_directory"], exist_ok=True)

feature_directories = config.get("feature_directory", [])
if isinstance(feature_directories, (str, Path)):
    feature_directories = [feature_directories]

# decide AF2 vs AF3 outputs
DATA_PIPELINE = config["create_feature_arguments"].get("--data_pipeline", "alphafold2").lower()
IS_AF3 = DATA_PIPELINE in ("alphafold3", "af3")

if IS_AF3:
    FEATURE_COMPRESSION = None
    FEATURE_SUFFIX = None
    FEATURE_OUTPUT_PATTERN = "{fasta_basename}_af3_input.json"
else:
    FEATURE_COMPRESSION = (
        "lzma" if config["create_feature_arguments"].get("--compress_features", False) else None
    )
    FEATURE_SUFFIX = feature_suffix(FEATURE_COMPRESSION)
    FEATURE_OUTPUT_PATTERN = "{fasta_basename}." + FEATURE_SUFFIX


def feature_name(base: str) -> str:
    return f"{base}_af3_input.json" if IS_AF3 else f"{base}.{FEATURE_SUFFIX}"

protein_delimiter = config.get("protein_delimiter", "+")
exclude_permutations = config.get("exclude_permutations", True)

input_files = config.get("input_files", [])
if isinstance(input_files, (str, Path)):
    input_files = [input_files]

DEFAULT_SLURM_PARTITION = config.get("slurm_partition")
DEFAULT_SLURM_GRES = config.get("slurm_gres")
DEFAULT_SLURM_QOS = config.get("slurm_qos")
STRUCTURE_INFERENCE_GPUS = int(config.get("structure_inference_gpus_per_task", 1))

prepare_container_binds(
    output_directory=config["output_directory"],
    config=config,
    feature_directories=feature_directories,
    input_files=input_files,
)

fold_specifications = generate_fold_specifications(
    input_files=input_files,
    delimiter=protein_delimiter,
    exclude_permutations=exclude_permutations,
)

dataset = FoldDataset.from_fold_specifications(
    fold_specifications=fold_specifications,
    protein_delimiter=protein_delimiter,
)

local_sequences = tuple(dataset.sequences_by_origin.get("local", ()))

ruleorder: symlink_local_files > download_uniprot > symlink_features > create_features

required_folds = [
    join(config["output_directory"], "predictions", fold, "completed_fold.txt")
    for fold in dataset.fold_specifications
]
ENABLE_STRUCTURE_ANALYSIS = config.get("enable_structure_analysis", True)
GENERATE_RECURSIVE_REPORT = config.get("generate_recursive_report", False)
RECURSIVE_REPORT_ARGUMENTS = config.get("recursive_report_arguments", {})
RECURSIVE_REPORT = (
    join(config["output_directory"], "reports", "all_interfaces.csv")
    if ENABLE_STRUCTURE_ANALYSIS and GENERATE_RECURSIVE_REPORT
    else None
)

required_feature_paths = [
    join(config["output_directory"], "features", feature_name(fasta_basename))
    for fasta_basename in dataset.unique_sequences
]
if config.get("only_generate_features", False):
    required_targets = required_feature_paths
else:
    if ENABLE_STRUCTURE_ANALYSIS:
        if GENERATE_RECURSIVE_REPORT and RECURSIVE_REPORT:
            required_targets = [RECURSIVE_REPORT]
        else:
            required_targets = [
                join(config["output_directory"], "predictions", fold, "interfaces.csv")
                for fold in dataset.fold_specifications
            ]
    else:
        required_targets = required_folds

precomputed_features = []
required_feature_basenames = {basename(x) for x in required_feature_paths}
for feature_directory in feature_directories:
    if not exists(feature_directory):
        continue
    available_features = listdir(feature_directory)
    for available_feature in available_features:
        if available_feature not in required_feature_basenames:
            continue
        precomputed_features.append(join(feature_directory, available_feature))

rule all:
    input:
        required_targets,

rule symlink_local_files:
    input:
        local_sequences,
    output:
        [
            join(
                config["output_directory"],
                "data",
                f"{splitext(basename(x))[0]}.fasta",
            )
            for x in local_sequences
        ],
    resources:
        qos=DEFAULT_SLURM_QOS,
        **linear_resources(mem=800, runtime=10),
    run:
        dataset.symlink_local_files(
            output_directory=join(config["output_directory"], "data")
        )

rule download_uniprot:
    output:
        join(config["output_directory"], "data", "{uniprot_id}.fasta"),
    resources:
        qos=DEFAULT_SLURM_QOS,
        **linear_resources(mem=800, runtime=10),
    shell:r"""
        set -euo pipefail
        temp_file=$(mktemp)
        curl -fsS -o "${{temp_file}}" "https://rest.uniprot.org/uniprotkb/{wildcards.uniprot_id}.fasta"
        echo ">{wildcards.uniprot_id}" > {output}
        tail -n +2 "${{temp_file}}" >> {output}
    """

feature_scaling = config.get("feature_create_ram_scaling", 1.1)
base_feature_ram = config.get("feature_create_ram_bytes", 64000)
feature_threads = config.get("feature_threads", 8)

rule symlink_features:
    input:
        precomputed_features,
    output:
        expand(
            join(config["output_directory"], "features", "{feature}"),
            feature=[basename(x) for x in precomputed_features],
        ),
    resources:
        qos=DEFAULT_SLURM_QOS,
        **linear_resources(mem=800, runtime=10),
    run:
        for in_file, out_file in zip(input, output):
            if exists(out_file):
                remove(out_file)
            symlink(abspath(in_file), out_file)

rule create_features:
    input:
        join(config["output_directory"], "data", "{fasta_basename}.fasta"),
    output:
        join(
            config["output_directory"],
            "features",
            FEATURE_OUTPUT_PATTERN,
        ),
    params:
        data_directory=config["databases_directory"],
        output_directory=join(config["output_directory"], "features"),
        cli_parameters=" ".join(
            [f"{k}={v}" for k, v in config["create_feature_arguments"].items()]
        ),
    resources:
        qos=DEFAULT_SLURM_QOS,
        **linear_resources(
            mem_fn=lambda wc, attempt: base_feature_ram * (feature_scaling ** attempt),
            runtime_fn=lambda wc, attempt: 1440 * attempt,
        ),
    threads: feature_threads,
    container:
        config["prediction_container"],
    shell:
        r"""
        create_individual_features.py \
            --fasta_paths={input} \
            --data_dir={params.data_directory} \
            --output_dir={params.output_directory} \
            {params.cli_parameters}
        """

def lookup_features(wildcards):
    return [
        join(config["output_directory"], "features", feature_name(feature))
        for feature in dataset.sequences_by_fold[wildcards.fold]
    ]

rule structure_inference:
    input:
        features=lookup_features,
    output:
        join(config["output_directory"], "predictions", "{fold}", "completed_fold.txt"),
    params:
        data_directory=config["backend_weights_directory"],
        feature_directory=join(config["output_directory"], "features"),
        output_directory=lambda wildcards: [
            join(config["output_directory"], "predictions", individual_fold)
            for individual_fold in wildcards.fold.split(" ")
        ],
        requested_fold = lambda wc: (
                protein_delimiter.join(f"{p}_af3_input.json" for p in wc.fold.split(protein_delimiter))
                if IS_AF3 else wc.fold
                ),
        protein_delimiter=protein_delimiter,
        cli_parameters=" ".join(
            [f"{k}={v}" for k, v in config["structure_inference_arguments"].items()]
        ),
    resources::
        slurm_partition=(DEFAULT_SLURM_PARTITION or "gpu"),
        gres=(DEFAULT_SLURM_GRES or f"gpu:{STRUCTURE_INFERENCE_GPUS}"),
        qos=DEFAULT_SLURM_QOS,
        **linear_resources(
            mem_fn=lambda wc, attempt: config.get("structure_inference_ram_bytes", 32000)
            * (1.1 ** attempt),
            runtime_fn=lambda wc, attempt: 1440 * attempt,
        ),
    threads:
        config["alphafold_inference_threads"],
    container:
        config["prediction_container"],
    shell:
        r"""
        run_structure_prediction.py \
            --input {params.requested_fold} \
            --output_directory={params.output_directory} \
            --protein_delimiter={params.protein_delimiter} \
            --data_directory={params.data_directory} \
            --features_directory={params.feature_directory} \
            {params.cli_parameters}

        echo "Completed" > "{output}"
        """

if ENABLE_STRUCTURE_ANALYSIS:
    rule analyze_structure:
        input:
            rules.structure_inference.output,
        output:
            join(config["output_directory"], "predictions", "{fold}", "interfaces.csv"),
        resources:
            qos=DEFAULT_SLURM_QOS,
            **linear_resources(mem=8000, runtime=1440),
        params:
            prediction_dir=lambda wildcards: join(
                config["output_directory"], "predictions", wildcards.fold
            ),
            fold=lambda wildcards: wildcards.fold,
            cli_parameters=" ".join(
                [f"{k}={v}" for k, v in config["analyze_structure_arguments"].items()]
            ),
        container:
            config["analysis_container"],
        shell:
            r"""
            alphajudge \
            {params.prediction_dir} \
            {params.cli_parameters}
            """


def _format_recursive_arguments(arguments) -> list[str]:
    if isinstance(arguments, dict):
        formatted = []
        for key, value in arguments.items():
            if isinstance(value, bool):
                if value:
                    formatted.append(str(key))
            elif value is None:
                formatted.append(str(key))
            else:
                formatted.append(f"{key}={value}")
        return formatted
    if isinstance(arguments, (list, tuple, set)):
        return [str(item) for item in arguments]
    if isinstance(arguments, str):
        return [arguments]
    return []


if ENABLE_STRUCTURE_ANALYSIS and GENERATE_RECURSIVE_REPORT:

    rule recursive_report:
        input:
                [
                    join(
                        config["output_directory"],
                        "predictions",
                        fold,
                        "interfaces.csv",
                    )
                    for fold in dataset.fold_specifications
                ],
        output:
                RECURSIVE_REPORT,
        params:
            prediction_dir=lambda wildcards: join(config["output_directory"], "predictions"),
            report_dir=join(config["output_directory"], "reports"),
            extra_args=lambda wildcards: " ".join(
                _format_recursive_arguments(RECURSIVE_REPORT_ARGUMENTS)
            ),
        resources:
            qos=DEFAULT_SLURM_QOS,
            **linear_resources(mem=2000, runtime=1440),
        container:
            config["analysis_container"],
        shell:
            r"""
                mkdir -p {params.report_dir}
                alphajudge \
                    {params.prediction_dir} \
                    --recursive \
                    --summary {output} {params.extra_args}
            """

