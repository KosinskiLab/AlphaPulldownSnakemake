# Snakefile: multi-run automated structure prediction

import os, re, tempfile
from os.path import abspath, join, basename, splitext, exists
from collections import defaultdict

include: "rules/common.smk"
configfile: "config/config.yaml"

# -----------------------------
# 0) config & helpers
# -----------------------------
RUNS = config["run"]  # list of {name, input, output}
assert isinstance(RUNS, list) and RUNS, "config['run'] must be a non-empty list of runs"

# compress features?
FEATURE_COMPRESSION = None
if config.get("create_feature_arguments", {}).get("--compress_features", False):
    FEATURE_COMPRESSION = "lzma"
FEATURE_SUFFIX = feature_suffix(FEATURE_COMPRESSION)

PROT_DELIM = config.get("protein_delimiter", ";")

# precomputed features: basename -> source path
PRECOMP_MAP = {}
for d in config.get("feature_directory", []):
    if os.path.isdir(d):
        for f in os.listdir(d):
            if f.endswith(f".{FEATURE_SUFFIX}"):
                PRECOMP_MAP[f] = abspath(join(d, f))

# convenience
def safe_name(name: str) -> str:
    return re.sub(r"[^A-Za-z0-9_]", "_", name)

# for the final "all" target
TOTAL_REQUIRED = []

# -----------------------------
# 1) parse each run & declare rules
# -----------------------------
for run in RUNS:
    run_name = run["name"]
    run_safe = safe_name(run_name)
    input_sheet = abspath(run["input"])
    run_dir = abspath(run["output"])
    os.makedirs(run_dir, exist_ok=True)

    # build dataset from the sheet using the same parser as single-run
    with tempfile.NamedTemporaryFile(mode="w+", delete=False) as tmp:
        process_files([input_sheet], tmp.name, delimiter=PROT_DELIM)
        DATASET = InputParser.from_file(
            filepath=tmp.name, file_format="alphaabriss", protein_delimiter=PROT_DELIM
        )

    # --------- per-run derived lists/maps ----------
    # sequences (feature basenames)
    seqs = list(DATASET.unique_sequences)
    folds = list(DATASET.fold_specifications)

    # where FASTA files will live
    fasta_path = lambda base: join(run_dir, "data", f"{base}.fasta")
    feat_path  = lambda base: join(run_dir, "features", f"{base}.{FEATURE_SUFFIX}")
    pred_done  = lambda fold: join(run_dir, "predictions", fold, "completed_fold.txt")
    fold_csv   = lambda fold: join(run_dir, "predictions", fold, "analysis.csv")
    report_csv = join(run_dir, "reports", "analysis.csv")
    report_html= join(run_dir, "reports", "report.html")
    cluster_txt= join(run_dir, "resources", "sequence_clusters.txt")

    # local file -> symlink mapping
    local_src = DATASET.sequences_by_origin.get("local", [])
    LOCAL_SYM_IN  = []
    LOCAL_SYM_OUT = []
    for src in local_src:
        base = splitext(basename(src))[0]
        LOCAL_SYM_IN.append(abspath(src))
        LOCAL_SYM_OUT.append(fasta_path(base))

    # precomputed features needed for THIS run
    PRECOMP_IN, PRECOMP_OUT = [], []
    for base in seqs:
        fname = f"{base}.{FEATURE_SUFFIX}"
        if fname in PRECOMP_MAP:
            PRECOMP_IN.append(PRECOMP_MAP[fname])
            PRECOMP_OUT.append(feat_path(base))

    # what this run must create
    if config.get("only_generate_features", False):
        TOTAL_REQUIRED += [feat_path(b) for b in seqs]
    else:
        TOTAL_REQUIRED += \
            [feat_path(b) for b in seqs] + \
            [pred_done(f) for f in folds] + \
            [report_csv, report_html]
        if config.get("cluster_jobs", False):
            TOTAL_REQUIRED.append(cluster_txt)

    # --------- rules (names are unique per run) ----------

    # local FASTA symlinks (multi-output, defined only if needed)
    if LOCAL_SYM_OUT:
        rule_name = f"symlink_local_fastas__{run_safe}"
        rule:
            name: rule_name
            input:  LOCAL_SYM_IN
            output: LOCAL_SYM_OUT
            resources:
                mem_mb = 100, walltime = 1, attempt = 1
            run:
                import os
                for src, dst in zip(input, output):
                    os.makedirs(os.path.dirname(dst), exist_ok=True)
                    if os.path.exists(dst):
                        os.remove(dst)
                    os.symlink(abspath(src), dst)

    # download FASTA by assumed UniProt ID (patterned per run, used when no local)
    rule_name = f"download_uniprot__{run_safe}"
    rule:
        name: rule_name
        output: join(run_dir, "data", "{fasta_basename}.fasta")
        resources:
            mem_mb = 100, walltime = 10, attempt = 1
        shell: r"""
            mkdir -p $(dirname {output})
            tmp=$(mktemp)
            curl -s -o "$tmp" "https://rest.uniprot.org/uniprotkb/{wildcards.fasta_basename}.fasta"
            echo ">{wildcards.fasta_basename}" > {output}
            tail -n +2 "$tmp" >> {output} || true
            rm -f "$tmp"
        """

    # precomputed feature symlinks (multi-output, defined only if some exist)
    if PRECOMP_OUT:
        rule_name = f"symlink_precomputed_features__{run_safe}"
        rule:
            name: rule_name
            input:  PRECOMP_IN
            output: PRECOMP_OUT
            resources:
                mem_mb = 100, walltime = 1, attempt = 1
            run:
                import os
                for src, dst in zip(input, output):
                    os.makedirs(os.path.dirname(dst), exist_ok=True)
                    if os.path.exists(dst):
                        os.remove(dst)
                    os.symlink(abspath(src), dst)

    # create missing features (patterned per run)
    rule_name = f"create_features__{run_safe}"
    rule:
        name: rule_name
        input: join(run_dir, "data", "{fasta_basename}.fasta")
        output: join(run_dir, "features", "{fasta_basename}." + FEATURE_SUFFIX)
        params:
            data_directory   = config["databases_directory"],
            output_directory = join(run_dir, "features"),
            cli_parameters   = " ".join(f"{k}={v}" for k,v in config.get("create_feature_arguments", {}).items())
        threads: 8
        container: config["prediction_container"]
        resources:
            mem_mb   = lambda wc, attempt: config.get("feature_create_ram_bytes", 64000) * (config.get("feature_create_ram_scaling",1.1)**attempt),
            walltime = lambda wc, attempt: 1440 * attempt,
            attempt  = lambda wc, attempt: attempt
        shell: r"""
            create_individual_features.py \
              --fasta_paths={input} \
              --data_dir={params.data_directory} \
              --output_dir={params.output_directory} \
              {params.cli_parameters}
        """

    # optional clustering checkpoint (per run)
    if config.get("cluster_jobs", False):
        rule_name = f"cluster_sequence_length__{run_safe}"
        checkpoint:
            name: rule_name
            input: [feat_path(b) for b in seqs]
            output: cluster_txt
            params:
                folds             = folds,
                protein_delimiter = PROT_DELIM,
                feature_directory = join(run_dir, "features"),
                cluster_bin_size  = config.get("clustering_bin_size", 150)
            container: config["prediction_container"]
            resources:
                mem_mb = 800, walltime = 10, attempt = 1
            script: "scripts/cluster_sequence_length.py"

        # utilities bound to this run
        def _lookup_features_factory(_run_dir, _dataset):
            def _lookup_features(wc):
                # if clustering, update mapping on demand
                if config.get("cluster_jobs", False):
                    path = join(_run_dir, "resources", "sequence_clusters.txt")
                    with open(path) as f:
                        data = [l.strip().split(",") for l in f if l.strip()]
                    hdr = data.pop(0)
                    table = {h:list(col) for h,col in zip(hdr, zip(*data))}
                    _dataset.update_clustering(data=table)
                return [join(_run_dir, "features", f"{feat}.{FEATURE_SUFFIX}")
                        for feat in _dataset.sequences_by_fold[wc.fold]]
            return _lookup_features

        def _format_clustering_factory(_run_dir):
            def _format_clustering(wc):
                s = ""
                if config.get("cluster_jobs", False):
                    path = join(_run_dir, "resources", "sequence_clusters.txt")
                    with open(path) as f:
                        data = [l.strip().split(",") for l in f if l.strip()]
                    hdr = data.pop(0)
                    table = {h:list(col) for h,col in zip(hdr, zip(*data))}
                    for name,length,depth in zip(table["name"], table["max_seq_length"], table["max_msa_depth"]):
                        if name == wc.fold.split(" ")[0]:
                            s += f"--desired_num_res={length} --desired_num_msa={depth} "
                return s
            return _format_clustering
        lookup_features = _lookup_features_factory(run_dir, DATASET)
        format_clustering = _format_clustering_factory(run_dir)
    else:
        # no clustering -> simple lookups
        def _lookup_features_factory(_run_dir, _dataset):
            def _lookup_features(wc):
                return [join(_run_dir, "features", f"{feat}.{FEATURE_SUFFIX}")
                        for feat in _dataset.sequences_by_fold[wc.fold]]
            return _lookup_features
        def _format_clustering_factory(_run_dir):
            def _format_clustering(wc): return ""
            return _format_clustering
        lookup_features = _lookup_features_factory(run_dir, DATASET)
        format_clustering = _format_clustering_factory(run_dir)

    # structure inference (patterned per run)
    rule_name = f"structure_inference__{run_safe}"
    rule:
        name: rule_name
        input:
            features = lookup_features,
            cluster  = (cluster_txt if config.get("cluster_jobs", False) else "/dev/null")
        output: join(run_dir, "predictions", "{fold}", "completed_fold.txt")
        params:
            data_directory     = config["backend_weights_directory"],
            feature_directory  = join(run_dir, "features"),
            output_directory   = lambda wc: " ".join([join(run_dir, "predictions", f) for f in wc.fold.split(" ")]),
            requested_fold     = lambda wc: wc.fold.replace(" ", ","),
            protein_delimiter  = PROT_DELIM,
            clustering_format  = format_clustering,
            cli_parameters     = " ".join(f"{k}={v}" for k,v in config.get("structure_inference_arguments", {}).items())
        threads: config["alphafold_inference_threads"]
        container: config["prediction_container"]
        resources:
            mem_mb   = lambda wc, attempt: config.get("structure_inference_ram_bytes", 32000) * (1.1**attempt),
            walltime = lambda wc, attempt: 1440 * attempt,
            attempt  = lambda wc, attempt: attempt
        shell: r"""
            run_structure_prediction.py \
              --input {params.requested_fold} \
              --output_directory={params.output_directory} \
              --protein_delimiter={params.protein_delimiter} \
              --data_directory={params.data_directory} \
              --features_directory={params.feature_directory} \
              {params.clustering_format} \
              {params.cli_parameters}
            echo "Completed" > "{output}"
        """

    # analyze single fold (patterned)
    rule_name = f"analyze_structure__{run_safe}"
    rule:
        name: rule_name
        input: join(run_dir, "predictions", "{fold}", "completed_fold.txt")
        output: join(run_dir, "predictions", "{fold}", "analysis.csv")
        params:
            prediction_dir = lambda wc: join(run_dir, "predictions", wc.fold),
            fold           = lambda wc: wc.fold,
            report_dir     = join(run_dir, "reports"),
            cli_parameters = " ".join(f"{k}={v}" for k,v in config.get("analyze_structure_arguments", {}).items())
        container: config["analysis_container"]
        resources:
            mem_mb = 8000, walltime = 1440, attempt = 1
        shell: r"""
            tmpdir=$(mktemp -d)
            cd "$tmpdir"
            ln -s {params.prediction_dir} "{params.fold}"
            get_good_inter_pae.py --output_dir="$tmpdir" {params.cli_parameters}
            mv predictions_with_good_interpae.csv {output}
            rm -rf "$tmpdir"
        """

    # merge analyses for this run (literal)
    rule_name = f"merge_analyses__{run_safe}"
    rule:
        name: rule_name
        input: [fold_csv(f) for f in folds]
        output: report_csv
        container: config["analysis_container"]
        resources:
            mem_mb = 2000, walltime = 1440, attempt = 1
        shell: r"""
            head -n 1 {input[0]} > {output}
            for f in {input}; do
                tail -n +2 "$f" >> {output}
            done
        """

    # final html report (literal; waits on completed folds)
    rule_name = f"generate_report__{run_safe}"
    rule:
        name: rule_name
        input: [pred_done(f) for f in folds]
        output: report_html
        params:
            prediction_dir = join(run_dir, "predictions"),
            report_dir     = join(run_dir, "reports"),
            cli_parameters = " ".join(f"{k}={v}" for k,v in config.get("generate_report_arguments", {}).items())
        container: config["prediction_container"]
        resources:
            mem_mb = 32000, walltime = 1440, attempt = 1
        shell: r"""
            mkdir -p {params.report_dir}
            cd {params.prediction_dir}
            create_notebook.py --output_dir={params.prediction_dir} {params.cli_parameters}
            jupyter nbconvert --to html --execute output.ipynb
            mv output.ipynb {params.report_dir}
            mv output.html {output}
        """

    # ruleorder: prefer local & precomputed symlinks where possible
    if LOCAL_SYM_OUT:
        ruleorder: f"symlink_local_fastas__{run_safe} > download_uniprot__{run_safe}"
    if PRECOMP_OUT:
        ruleorder: f"symlink_precomputed_features__{run_safe} > create_features__{run_safe}"

# -----------------------------
# 2) the global 'all' target
# -----------------------------
rule all:
    input: TOTAL_REQUIRED
