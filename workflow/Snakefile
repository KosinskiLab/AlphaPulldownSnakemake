""" Snakemake pipeline for automated structure prediction using various backends.

    Copyright (c) 2024 European Molecular Biology Laboratory

    Authors: Valentin Maurer, Dingquan Yu <name.surname@embl-hamburg.de>
"""
import os
import tempfile
from os.path import abspath, join, basename, splitext

include: "rules/common.smk"
configfile: "config/config.yaml"

RUNS = config["run"]
NAMES = [r["name"] for r in RUNS]
INPUT_MAP = {r["name"]: r["input"] for r in RUNS}
OUT_MAP = {r["name"]: abspath(r["output"]) for r in RUNS}

for d in OUT_MAP.values():
    os.makedirs(d, exist_ok=True)

FEATURE_COMPRESSION = None
if config["create_feature_arguments"].get("--compress_features", False):
    FEATURE_COMPRESSION = "lzma"
FEATURE_SUFFIX = feature_suffix(FEATURE_COMPRESSION)
protein_delimiter = config.get("protein_delimiter", "_")

def load_dataset(wc):
    return InputParser.from_file(
        filepath=f"tmp/{wc.name}.parsed",
        file_format="alphaabriss",
        protein_delimiter=protein_delimiter
    )

rule parse_input:
    output:
        temp("tmp/{name}.parsed")
    params:
        csv=lambda wc: INPUT_MAP[wc.name],
        od=lambda wc: join(OUT_MAP[wc.name], "data")
    run:
        os.makedirs(params.od, exist_ok=True)
        with tempfile.NamedTemporaryFile("w+", delete=False) as tmp:
            process_files(input_files=[params.csv], output_path=tmp.name, delimiter=protein_delimiter)
            import shutil
            shutil.copy(tmp.name, output[0])

rule all:
    input:
        expand(join(OUT_MAP["{name}"], "features", "{name}." + FEATURE_SUFFIX), name=NAMES),
        expand(join(OUT_MAP["{name}"], "predictions", "{fold}", "completed_fold.txt"), 
               name=NAMES, 
               fold=lambda wc: load_dataset(wc).fold_specifications),
        expand(join(OUT_MAP["{name}"], "reports", "report.html"), name=NAMES)

rule symlink_local_files:
    input:
        lambda wc: load_dataset(wc).sequences_by_origin["local"]
    output:
        lambda wc: [
            join(OUT_MAP[wc.name], "data", f"{splitext(basename(x))[0]}.fasta")
            for x in load_dataset(wc).sequences_by_origin["local"]
        ]
    run:
        load_dataset(wc).symlink_local_files(output_directory=join(OUT_MAP[wc.name], "data"))

rule download_uniprot:
    output:
        join(OUT_MAP["{name}"], "data", "{uniprot_id}.fasta")
    shell:
        """
        temp_file=$(mktemp)
        curl -o ${temp_file} https://rest.uniprot.org/uniprotkb/{wildcards.uniprot_id}.fasta
        echo ">{wildcards.uniprot_id}" > {output}
        tail -n +2 ${temp_file} >> {output}
        """

rule symlink_features:
    input:
        lambda wc: [
            f for d in config.get("feature_directory", []) if os.path.isdir(d)
            for f in [join(d, x) for x in os.listdir(d)]
            if basename(f) in {basename(x) for x in load_dataset(wc).unique_sequences}
        ]
    output:
        lambda wc: [
            join(OUT_MAP[wc.name], "features", basename(x)) for x in input
        ]
    run:
        for src, dst in zip(input, output):
            if os.path.exists(dst):
                os.remove(dst)
            os.symlink(abspath(src), dst)

rule create_features:
    input:
        join(OUT_MAP["{name}"], "data", "{fasta_basename}.fasta")
    output:
        join(OUT_MAP["{name}"], "features", "{fasta_basename}." + FEATURE_SUFFIX)
    params:
        data_dir=config["alphafold_data_directory"],
        out_dir=lambda wc: join(OUT_MAP[wc.name], "features"),
        cli=lambda wc: " ".join(f"{k}={v}" for k,v in config["create_feature_arguments"].items())
    threads:
        config["alphafold_inference_threads"]
    container:
        config["prediction_container"]
    shell:
        """
        create_individual_features.py --fasta_paths={input} --data_dir={params.data_dir} \
        --output_dir={params.out_dir} {params.cli}
        """

checkpoint cluster_sequence_length:
    input:
        lambda wc: [
            join(OUT_MAP[wc.name], "features", f"{f}.{FEATURE_SUFFIX}")
            for f in load_dataset(wc).unique_sequences
        ]
    output:
        join(OUT_MAP["{name}"], "resources", "sequence_clusters.txt")
    params:
        folds=lambda wc: load_dataset(wc).fold_specifications,
        feature_directory=lambda wc: join(OUT_MAP[wc.name], "features"),
        cluster_bin_size=config.get("clustering_bin_size", 150)
    container:
        config["prediction_container"]
    script:
        "scripts/cluster_sequence_length.py"

def lookup_features(wildcards):
    # Fixed: Use wildcards instead of wc for consistency
    name = wildcards.name
    fold = wildcards.fold
    
    if config.get("cluster_jobs", False):
        checkpoint_output = checkpoints.cluster_sequence_length.get(name=name).output[0]
        with open(checkpoint_output) as f:
            data = [x.strip().split(",") for x in f.read().splitlines() if x]
        headers = data.pop(0)
        ret = {h: list(col) for h, col in zip(headers, zip(*data))}
        dataset = load_dataset(wildcards)
        dataset.update_clustering(data=ret)
        sequences = dataset.sequences_by_fold[fold]
    else:
        dataset = load_dataset(wildcards)
        sequences = dataset.sequences_by_fold[fold]
        
    return [
        join(OUT_MAP[name], "features", f"{seq}.{FEATURE_SUFFIX}")
        for seq in sequences
    ]

rule structure_inference:
    input:
        features=lookup_features,
        cluster=lambda wc: join(OUT_MAP[wc.name], "resources", "sequence_clusters.txt") if config.get("cluster_jobs", False) else "/dev/null"
    output:
        join(OUT_MAP["{name}"], "predictions", "{fold}", "completed_fold.txt")
    params:
        data_dir=config["alphafold_data_directory"],
        feature_dir=lambda wc: join(OUT_MAP[wc.name], "features"),
        out_dirs=lambda wc: [join(OUT_MAP[wc.name], "predictions", f) for f in wc.fold.split(" ")],
        req_fold=lambda wc: wc.fold.replace(" ", ","),
        cli=lambda wc: " ".join(f"{k}={v}" for k,v in config["structure_inference_arguments"].items()),
        cluster_fmt=lambda wc: ""
    threads:
        config["alphafold_inference_threads"]
    container:
        config["prediction_container"]
    shell:
        """
        run_structure_prediction.py \
        --input {params.req_fold} \
        --output_directory={params.out_dirs} \
        --protein_delimiter={protein_delimiter} \
        --data_directory={params.data_dir} \
        --features_directory={params.feature_dir} \
        {params.cluster_fmt} {params.cli}; echo Completed > {output}
        """

rule analyze_structure:
    input:
        rules.structure_inference.output
    output:
        join(OUT_MAP["{name}"], "predictions", "{fold}", "analysis.csv")
    params:
        pred_dir=lambda wc: join(OUT_MAP[wc.name], "predictions", wc.fold),
        cli=lambda wc: " ".join(f"{k}={v}" for k,v in config["analyze_structure_arguments"].items())
    container:
        config["analysis_container"]
    shell:
        """
        tmp=$(mktemp -d); cd $tmp
        ln -s {params.pred_dir} $tmp/{wildcards.fold}
        get_good_inter_pae.py --output_dir=$tmp {params.cli}
        mv predictions_with_good_interpae.csv {output}
        """

rule merge_analyses:
    input:
        lambda wc: [
            join(OUT_MAP[wc.name], "predictions", f, "analysis.csv")
            for f in load_dataset(wc).fold_specifications
        ]
    output:
        join(OUT_MAP["{name}"], "reports", "analysis.csv")
    shell:
        """
        head -n 1 {input[0]} > {output}
        for f in {input}; do tail -n +2 $f >> {output}; done
        """

rule generate_report:
    input:
        lambda wc: [
            join(OUT_MAP[wc.name], "resources", "sequence_clusters.txt")
        ] if config.get("cluster_jobs", False) else []
    output:
        join(OUT_MAP["{name}"], "reports", "report.html")
    params:
        pred_dir=lambda wc: join(OUT_MAP[wc.name], "predictions"),
        cli=lambda wc: " ".join(f"{k}={v}" for k,v in config["generate_report_arguments"].items())
    container:
        config["prediction_container"]
    shell:
        """
        cd {params.pred_dir}
        create_notebook.py --output_dir={params.pred_dir} {params.cli}
        jupyter nbconvert --to html --execute output.ipynb
        mv output.html {output}
        """
