""" Snakemake pipeline for automated structure prediction using various backends.

    Copyright (c) 2024 European Molecular Biology Laboratory

    Authors: Valentin Maurer, Dingquan Yu <name.surname@embl-hamburg.de>
"""
import tempfile

from sys import exit
from os import makedirs, listdir, symlink, remove
from os.path import abspath, join, splitext, basename, exists

include: "rules/common.smk"

configfile: "config/config.yaml"
config["output_directory"] = abspath(config["output_directory"])
makedirs(config["output_directory"], exist_ok=True)

# decide AF2 vs AF3 outputs
DATA_PIPELINE = config["create_feature_arguments"].get("--data_pipeline", "alphafold2").lower()
IS_AF3 = DATA_PIPELINE in ("alphafold3", "af3")

# AF2 keeps optional compression; AF3 forces plain JSON filenames
FEATURE_COMPRESSION = None
if (not IS_AF3) and config["create_feature_arguments"].get("--compress_features", False):
    FEATURE_COMPRESSION = "lzma"

# feature_suffix() comes from rules/common.smk
FEATURE_SUFFIX = feature_suffix(FEATURE_COMPRESSION) if not IS_AF3 else None

def feature_name(base: str) -> str:
    return f"{base}_af3_input.json" if IS_AF3 else f"{base}.{FEATURE_SUFFIX}"

# IMPORTANT: output patterns must be static strings (no lambdas)
FEATURE_OUTPUT_PATTERN = (
    "{fasta_basename}_af3_input.json" if IS_AF3 else "{fasta_basename}." + FEATURE_SUFFIX
)

protein_delimiter = config.get("protein_delimiter", ";")

with tempfile.NamedTemporaryFile(mode="w+", delete=False) as tmp_input:
    input_files = config["input_files"]
    if isinstance(input_files, str):
        input_files = [input_files]

    process_files(
        input_files=input_files,
        output_path=tmp_input.name,
        delimiter=protein_delimiter,
    )

    dataset = InputParser.from_file(
        filepath=tmp_input.name,
        file_format="alphaabriss",
        protein_delimiter=protein_delimiter,
    )

ruleorder: symlink_local_files > download_uniprot > symlink_features > create_features

required_folds = [
    join(config["output_directory"], "predictions", fold, "completed_fold.txt")
    for fold in dataset.fold_specifications
]
required_reports = [
    join(config["output_directory"], "reports", "analysis.csv"),
    join(config["output_directory"], "reports", "report.html"),
]
total_required_files = [*required_reports]

required_features = [
    join(config["output_directory"], "features", feature_name(fasta_basename))
    for fasta_basename in dataset.unique_sequences
]
if config.get("only_generate_features", False):
    total_required_files = required_features

if config.get("cluster_jobs", False):
    total_required_files.append(
        join(config["output_directory"], "resources", "sequence_clusters.txt")
    )

precomputed_features = []
required_features = set(basename(x) for x in required_features)
feature_directories = config.get("feature_directory", [])
for feature_directory in feature_directories:
    if not exists(feature_directory):
        continue
    available_features = listdir(feature_directory)
    for available_feature in available_features:
        if available_feature not in required_features:
            continue
        precomputed_features.append(join(feature_directory, available_feature))

rule all:
    input:
        total_required_files,

rule symlink_local_files:
    input:
        dataset.sequences_by_origin["local"],
    output:
        [
            join(
                config["output_directory"],
                "data",
                f"{splitext(basename(x))[0]}.fasta",
            )
            for x in dataset.sequences_by_origin["local"]
        ],
    resources:
        avg_mem=lambda wildcards, attempt: 600 * attempt,
        mem_mb=lambda wildcards, attempt: 800 * attempt,
        walltime=lambda wildcards, attempt: 10 * attempt,
        attempt=lambda wildcards, attempt: attempt,
    run:
        dataset.symlink_local_files(
            output_directory=join(config["output_directory"], "data")
        )

rule download_uniprot:
    output:
        join(config["output_directory"], "data", "{uniprot_id}.fasta"),
    resources:
        avg_mem=lambda wildcards, attempt: 600 * attempt,
        mem_mb=lambda wildcards, attempt: 800 * attempt,
        walltime=lambda wildcards, attempt: 10 * attempt,
        attempt=lambda wildcards, attempt: attempt,
    shell:r"""
        set -euo pipefail
        temp_file=$(mktemp)
        curl -fsS -o "${{temp_file}}" "https://rest.uniprot.org/uniprotkb/{wildcards.uniprot_id}.fasta"
        echo ">{wildcards.uniprot_id}" > {output}
        tail -n +2 "${{temp_file}}" >> {output}
    """

feature_scaling = config.get("feature_create_ram_scaling", 1.1)
base_feature_ram = config.get("feature_create_ram_bytes", 64000)

rule symlink_features:
    input:
        precomputed_features,
    output:
        expand(
            join(config["output_directory"], "features", "{feature}"),
            feature=[basename(x) for x in precomputed_features],
        ),
    resources:
        avg_mem=lambda wildcards, attempt: 600 * attempt,
        mem_mb=lambda wildcards, attempt: 800 * attempt,
        walltime=lambda wildcards, attempt: 10 * attempt,
        attempt=lambda wildcards, attempt: attempt,
    run:
        for in_file, out_file in zip(input, output):
            if exists(out_file):
                remove(out_file)
            symlink(abspath(in_file), out_file)

rule create_features:
    input:
        join(config["output_directory"], "data", "{fasta_basename}.fasta"),
    output:
        join(
            config["output_directory"],
            "features",
            FEATURE_OUTPUT_PATTERN,
        ),
    params:
        data_directory=config["databases_directory"],
        output_directory=join(config["output_directory"], "features"),
        cli_parameters=" ".join(
            [f"{k}={v}" for k, v in config["create_feature_arguments"].items()]
        ),
    resources:
        mem_mb=lambda wildcards, attempt: base_feature_ram * (feature_scaling ** attempt),
        walltime=lambda wildcards, attempt: 1440 * attempt,
        attempt=lambda wildcards, attempt: attempt,
    threads: 8,  # everything is hardcoded anyways ...
    container:
        config["prediction_container"],
    shell:
        r"""
        create_individual_features.py \
            --fasta_paths={input} \
            --data_dir={params.data_directory} \
            --output_dir={params.output_directory} \
            {params.cli_parameters}
        """

checkpoint cluster_sequence_length:
    input:
        [
            join(config["output_directory"], "features", feature_name(feature))
            for feature in dataset.unique_sequences
        ],
    output:
        join(config["output_directory"], "resources", "sequence_clusters.txt"),
    resources:
        avg_mem=lambda wildcards, attempt: 600 * attempt,
        mem_mb=lambda wildcards, attempt: 800 * attempt,
        walltime=lambda wildcards, attempt: 10 * attempt,
        attempt=lambda wildcards, attempt: attempt,
    params:
        folds=dataset.fold_specifications,
        protein_delimiter=protein_delimiter,
        feature_directory=join(config["output_directory"], "features"),
        cluster_bin_size=config.get("clustering_bin_size", 150),
    container:
        config["prediction_container"],
    script:
        "scripts/cluster_sequence_length.py"

def lookup_features(wildcards):
    if config.get("cluster_jobs", False):
        with checkpoints.cluster_sequence_length.get().output[0].open() as f:
            data = [x.strip().split(",") for x in f.read().split("\n") if len(x.strip())]
        headers = data.pop(0)
        ret = {header: list(column) for header, column in zip(headers, zip(*data))]
        dataset.update_clustering(data=ret)

    return [
        join(config["output_directory"], "features", feature_name(feature))
        for feature in dataset.sequences_by_fold[wildcards.fold]
    ]

def format_clustering(wildcards):
    parameter_string = ""
    if config.get("cluster_jobs", False):
        with checkpoints.cluster_sequence_length.get().output[0].open() as f:
            data = [x.strip().split(",") for x in f.read().split("\n") if len(x.strip())]
        headers = data.pop(0)
        ret = {header: list(column) for header, column in zip(headers, zip(*data))}
        fold = wildcards.fold.split(" ")[0]
        for name, length, depth in zip(ret["name"], ret["max_seq_length"], ret["max_msa_depth"]):
            if name == fold:
                parameter_string += f"--desired_num_res={length} "
                parameter_string += f"--desired_num_msa={depth} "
    return parameter_string

def requested_input(wildcards):
    # AF3: pass filenames (relative to --features_directory)
    if IS_AF3:
        names = [feature_name(f) for f in dataset.sequences_by_fold[wildcards.fold]]
        return ",".join(names)
    # AF2: pass bare IDs
    return wildcards.fold.replace(" ", ",")

rule structure_inference:
    input:
        features=lookup_features,
        cluster=join(config["output_directory"], "resources", "sequence_clusters.txt")
        if config.get("cluster_jobs", False) else "/dev/null",
    output:
        join(config["output_directory"], "predictions", "{fold}", "completed_fold.txt"),
    params:
        data_directory=config["backend_weights_directory"],
        feature_directory=join(config["output_directory"], "features"),
        output_directory=lambda wildcards: [
            join(config["output_directory"], "predictions", individual_fold)
            for individual_fold in wildcards.fold.split(" ")
        ],
        requested_input=requested_input,
        protein_delimiter=protein_delimiter,
        cli_parameters=" ".join(
            [f"{k}={v}" for k, v in config["structure_inference_arguments"].items()]
        ),
        clustering_format=format_clustering,
    resources:
        mem_mb=lambda wildcards, attempt: config.get("structure_inference_ram_bytes", 32000) * (1.1 ** attempt),
        walltime=lambda wildcards, attempt: 1440 * attempt,
        attempt=lambda wildcards, attempt: attempt,
        slurm=config.get("alphafold_inference", ""),
    threads:
        config["alphafold_inference_threads"],
    container:
        config["prediction_container"],
    shell:
        r"""
        run_structure_prediction.py \
            --input {params.requested_input} \
            --output_directory={params.output_directory} \
            --protein_delimiter={params.protein_delimiter} \
            --data_directory={params.data_directory} \
            --features_directory={params.feature_directory} \
            {params.clustering_format} \
            {params.cli_parameters}

        echo "Completed" > "{output}"
        """

def update_clustering(wildcards):
    if config.get("cluster_jobs", False):
        with checkpoints.cluster_sequence_length.get().output[0].open() as f:
            data = [x.strip().split(",") for x in f.read().split("\n") if len(x.strip())]
        headers = data.pop(0)
        ret = {header: list(column) for header, column in zip(headers, zip(*data))]
        dataset.update_clustering(data=ret)

    return [
        join(config["output_directory"], "predictions", fold, "completed_fold.txt")
        for fold in dataset.fold_specifications
    ]

rule analyze_structure:
    input:
        rules.structure_inference.output,
    output:
        join(config["output_directory"], "predictions", "{fold}", "analysis.csv"),
    resources:
        mem_mb=lambda wildcards, attempt: 8000 * attempt,
        walltime=lambda wildcards, attempt: 1440 * attempt,
        attempt=lambda wildcards, attempt: attempt,
    params:
        prediction_dir=lambda wildcards: join(
            config["output_directory"], "predictions", wildcards.fold
        ),
        fold=lambda wildcards: wildcards.fold,
        report_dir=join(config["output_directory"], "reports"),
        cli_parameters=" ".join(
            [f"{k}={v}" for k, v in config["analyze_structure_arguments"].items()]
        ),
    container:
        config["analysis_container"],
    shell:
        r"""
        tmpdir=$(mktemp -d)
        cd "$tmpdir"
        ln -s {params.prediction_dir} "$tmpdir/{params.fold}"
        get_good_inter_pae.py \
            --output_dir="$tmpdir" \
            {params.cli_parameters}
        touch predictions_with_good_interpae.csv
        mv predictions_with_good_interpae.csv {output}
        rm -rf "$tmpdir"
        """

def update_statistics(wildcards):
    return [
        join(config["output_directory"], "predictions", fold, "analysis.csv")
        for fold in dataset.fold_specifications
    ]

rule merge_analyses:
    input:
        update_statistics,
    output:
        join(config["output_directory"], "reports", "analysis.csv"),
    resources:
        mem_mb=lambda wildcards, attempt: 2000 * attempt,
        walltime=lambda wildcards, attempt: 1440 * attempt,
        attempt=lambda wildcards, attempt: attempt,
    params:
        prediction_dir=join(config["output_directory"], "predictions"),
    container:
        config["analysis_container"],
    shell:
        r"""
        head -n 1 {input[0]} > {output}
        for f in {input}; do
            tail -n +2 "$f" >> {output}
        done
        """

rule generate_report:
    input:
        update_clustering,
    output:
        join(config["output_directory"], "reports", "report.html"),
    resources:
        mem_mb=lambda wildcards, attempt: 32000 * attempt,
        walltime=lambda wildcards, attempt: 1440 * attempt,
        attempt=lambda wildcards, attempt: attempt,
    params:
        prediction_dir=join(config["output_directory"], "predictions"),
        report_dir=join(config["output_directory"], "reports"),
        cli_parameters=" ".join(
            [f"{k}={v}" for k, v in config["generate_report_arguments"].items()]
        ),
    container:
        config["prediction_container"],
    shell:
        r"""
        cd {params.prediction_dir}
        create_notebook.py \
            --output_dir={params.prediction_dir} \
            {params.cli_parameters}
        jupyter nbconvert --to html --execute output.ipynb
        mv output.ipynb {params.report_dir}
        mv output.html {output}
        """
